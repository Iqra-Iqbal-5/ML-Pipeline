# -*- coding: utf-8 -*-
"""ML Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/120BKMY7kxxltneKaqBQr53sAPaBHA0Vu
"""

!pip install ptitprince
! pip install mpld3
! pip install tkinter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import ptitprince as pt
from IPython.display import display, Markdown
import numpy as np
from sklearn.preprocessing import PowerTransformer
from scipy.special import boxcox1p
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression
from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV, KFold, StratifiedKFold, LeaveOneOut
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor,GradientBoostingClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler, Normalizer, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report
import ipywidgets as widgets
from IPython.display import display, Markdown,HTML,FileLink
import mpld3
import base64
import io
from scipy.stats import skew # Import the skew function
from scipy import stats
from scipy.stats import zscore, iqr
from scipy.stats.mstats import winsorize
from tkinter import Tk, filedialog
import warnings
warnings.filterwarnings('ignore')
from scipy import stats
from scipy.stats import zscore, iqr
import matplotlib.pyplot as plt
from scipy.stats.mstats import winsorize
import seaborn as sns
import ptitprince as pt
import plotly.express as px
import plotly.graph_objects as go
import warnings
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Global variables to hold data, model, and scalers
model = None
label_encoder = None
feature_widgets = {}

# Step 1: Data Collection
# Create a file upload widget
upload_button = widgets.FileUpload(description="Upload CSV", accept='.csv')
data = None
def update_feature_dropdown(change):
    if data is not None:
        feature_dropdown.options = [col for col in data.columns if col != target_input.value]
# Function to load the dataset
def load_dataset(change):
    global data
    uploaded_file = upload_button.value
    if uploaded_file:
        # Get the first (and presumably only) uploaded file
        file_name = next(iter(uploaded_file))
        file_content = uploaded_file[file_name]['content']
        data = pd.read_csv(io.BytesIO(file_content))
        display(Markdown("### Dataset Information"))
        display(Markdown(f"**Number of instances:** {data.shape[0]}"))
        display(Markdown(f"**Number of features:** {data.shape[1]}"))
        display(Markdown("### First 5 Rows of the Dataset"))
        display(data.head())
        # Automatically select the last column as the target
        target_input.options = data.columns.tolist()
        target_input.value = None  # Clear the target input initially
        feature_dropdown.options = []  # Clear feature dropdown initially
        columns_to_drop.options = data.columns.tolist()
        columns_to_fill.options = data.columns[data.isnull().any()].tolist()
        categorical_columns.options = data.select_dtypes(include=['object', 'category']).columns.tolist()
        update_missing_values_info()
        update_feature_widgets()

# Attach the load_dataset function to the file upload button
upload_button.observe(load_dataset, names='value')
# Display the upload button
display(Markdown("## Step 1: Data Collection"))
display(upload_button)
target_column_dropdown = widgets.Dropdown(
    options=[],  # Initially no options until data is loaded
    description='Target Column:',
    disabled=False,
)

# New Dropdown for Additional Data Information
data_info_dropdown = widgets.Dropdown(
    options=[
        'Describe Data'
    ],
    description='Data Info:',
    disabled=False,
)
# Function to handle data information options
def show_data_info(change):
    global data
    if data is not None:
        option = data_info_dropdown.value
        if option == 'Describe Data':
            display(Markdown(f"### Describe Data"))
            display(data.describe())
# Attach the show_data_info function to the data info dropdown
data_info_dropdown.observe(show_data_info, names='value')

# Display the data info dropdown
display(Markdown("###  Data Information"))
display(data_info_dropdown)

# Step 2: Visualizing Graphs

graph_type_dropdown = widgets.Dropdown(
    options=['Line Chart', 'Bar Chart', 'Column Chart', 'Pie Chart', 'Scatter Plot',
             'Histogram', 'Box Plot', 'Area Chart', 'Bubble Chart', 'Heatmap'],
    description='Graph Type:',
    disabled=False,
)
# Step 3: Data Preprocessing
# Create a dropdown widget for selecting the feature to visualize and use for modeling
feature_dropdown = widgets.SelectMultiple(
    options=[],
    description='Feature:',
    disabled=False,
)

target_input = widgets.Dropdown(
    options=[],
    value=None,
    description='Target:',
    disabled=False,
)
target_input.observe(update_feature_dropdown, names='value')
# Function to visualize different types of graphs
def visualize_graph(b):
    global data
    graph_type = graph_type_dropdown.value
    target_column = target_column_dropdown.value

    for column in data.columns:
        if column == target_column:
            continue

        plt.figure(figsize=(12, 6))

        if graph_type == 'Line Chart':
            plt.plot(data[column], data[target_column])
            plt.title(f'Line Chart of {column} vs {target_column}')
            plt.xlabel(column)
            plt.ylabel(target_column)

        elif graph_type == 'Bar Chart':
            data.groupby(column)[target_column].mean().plot(kind='bar')
            plt.title(f'Bar Chart of {column} vs {target_column}')
            plt.xlabel(column)
            plt.ylabel(target_column)

        elif graph_type == 'Column Chart':
            data.groupby(column)[target_column].mean().plot(kind='bar', color='orange')
            plt.title(f'Column Chart of {column} vs {target_column}')
            plt.xlabel(column)
            plt.ylabel(target_column)

        elif graph_type == 'Pie Chart':
            data[column].value_counts().plot(kind='pie', autopct='%1.1f%%')
            plt.title(f'Pie Chart of {column}')

        elif graph_type == 'Scatter Plot':
            plt.scatter(data[column], data[target_column])
            plt.title(f'Scatter Plot of {column} vs {target_column}')
            plt.xlabel(column)
            plt.ylabel(target_column)

        elif graph_type == 'Histogram':
            plt.hist(data[column].dropna(), bins=30)
            plt.title(f'Histogram of {column}')
            plt.xlabel(column)
            plt.ylabel('Frequency')

        elif graph_type == 'Box Plot':
            sns.boxplot(x=data[column])
            plt.title(f'Box Plot of {column}')

        elif graph_type == 'Area Chart':
            plt.fill_between(data.index, data[column])
            plt.title(f'Area Chart of {column}')
            plt.xlabel('Index')
            plt.ylabel(column)

        elif graph_type == 'Bubble Chart':
            if len(data.columns) > 2:
                plt.scatter(data[column], data[target_column], s=data[data.columns[2]]*10)
                plt.title(f'Bubble Chart of {column} vs {target_column}')
                plt.xlabel(column)
                plt.ylabel(target_column)
            else:
                display(Markdown("### Error: Bubble Chart requires at least three columns."))

        elif graph_type == 'Heatmap':
            corr = data.corr()
            sns.heatmap(corr, annot=True, cmap='coolwarm')
            plt.title('Heatmap of Correlation Matrix')

        plt.show()

# Button to generate the selected graph
graph_button = widgets.Button(description="Generate Graph")
graph_button.on_click(visualize_graph)

# Display the graph type dropdown and button
display(Markdown("## Step 2: Visualizing Graphs"))
display(graph_type_dropdown)
display(graph_button)

# Create a dropdown widget for selecting the preprocessing method
preprocess_dropdown = widgets.Dropdown(
    options=['None', 'Standard Scaler', 'Min-Max Scaler', 'Robust Scaler','Max-Abs Scaler',
        'Normalize to Unit Length'],
    value='None',
    description='Preprocess:',
    disabled=False,
)
import ipywidgets as widgets
from IPython.display import display, Markdown
import numpy as np
from sklearn.preprocessing import PowerTransformer
from scipy.special import boxcox1p

# Sample dataset X for demonstration (replace this with your actual dataset)
X = np.random.rand(10, 3)  # Sample data


target_column_dropdown = widgets.Dropdown(
    options=[], # Initially no options until data is loaded
    description='Target Column:',
    disabled=False,
)

# Create a widget to select columns to drop
columns_to_drop = widgets.SelectMultiple(
    options=[],
    description='Drop Columns:',
    disabled=False,
)

# Create a widget to select columns to fill missing values
columns_to_fill = widgets.SelectMultiple(
    options=[],
    description='Fill Columns:',
    disabled=False,
)
# Create a dropdown for filling method
fill_method_dropdown = widgets.Dropdown(
    options=['Mean', 'Median', 'Mode', 'Forward Fill', 'Backward Fill'],
    value='Mean',
    description='Fill Method:',
    disabled=False,
)

# Create a widget to select categorical columns
categorical_columns = widgets.SelectMultiple(
    options=[],
    description='Categorical Columns:',
    disabled=False,
)

# Create a dropdown for selecting the encoding method
encoding_method_dropdown = widgets.Dropdown(
    options=['Label Encoding', 'One-Hot Encoding'],
    value='Label Encoding',
    description='Encoding Method:',
    disabled=False,
)

# Function to apply the selected preprocessing method
# Function to apply the selected preprocessing method
# Function to visualize the scaling techniques using histograms
def visualize_scaling_techniques(b):
    global data
    features = data.columns[:-1]  # All columns except the target
    preprocessors = {
        'None': None,
        'Standard Scaler': StandardScaler(),
        'Min-Max Scaler': MinMaxScaler(),
        'Robust Scaler': RobustScaler()
    }

    for feature in features:
        fig, axes = plt.subplots(1, len(preprocessors), figsize=(20, 5))
        fig.suptitle(f'Scaling Techniques for Feature: {feature}', fontsize=16)

        for ax, (name, scaler) in zip(axes, preprocessors.items()):
            if scaler:
                scaled_data = scaler.fit_transform(data[[feature]])
                ax.hist(scaled_data, bins=20, alpha=0.7, label=f'{name}')
            else:
                ax.hist(data[feature], bins=20, alpha=0.7, label='Original')
            ax.set_title(name)
            ax.legend()

        plt.show()

# Button to visualize scaling techniques
visualize_scaling_button = widgets.Button(description="Visualize Scaling Techniques")
visualize_scaling_button.on_click(visualize_scaling_techniques)
def apply_preprocessing(b):
    global data  # Ensure X is in global scope to modify it
    preprocess = preprocess_dropdown.value

    if preprocess == 'Standard Scaler':
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data) # Scale the data and store in new variable
    elif preprocess == 'Min-Max Scaler':
        scaler = MinMaxScaler()
        data_scaled = scaler.fit_transform(data)
    elif preprocess == 'Robust Scaler':
        scaler = RobustScaler()
        data_scaled = scaler.fit_transform(data)
    elif preprocess == 'Max-Abs Scaler':
        scaler = MaxAbsScaler()
        data_scaled = scaler.fit_transform(data)
    elif preprocess == 'Normalize to Unit Length':
        normalizer = Normalizer()
        data_scaled = normalizer.fit_transform(data)
    else:
        data_scaled = data.values # If no preprocessing, just extract the values

    # Convert the scaled data back to a Pandas DataFrame, preserving column names
    data = pd.DataFrame(data_scaled, columns=data.columns)

    display(Markdown(f"### Applied {preprocess}"))
    display(data)

# Create a button to apply the preprocessing
apply_preprocessing_button = widgets.Button(description="Apply Preprocessing")
apply_preprocessing_button.on_click(apply_preprocessing)

# Display the dropdown and button
display(visualize_scaling_button)
display(preprocess_dropdown)
display(apply_preprocessing_button)
# Function to update missing values information
def update_missing_values_info():
    if data is not None:
        missing_info = data.isnull().sum()
        missing_info = missing_info[missing_info > 0]
        if not missing_info.empty:
            display(Markdown("### Missing Values Information"))
            display(missing_info)

# Function to visualize the dataset
def visualize_data(change=None):
    feature = feature_dropdown.value
    target = target_input.value
    if feature and target:
        X = data[feature].values  # Feature
        y = data[target].values  # Target

        plt.figure(figsize=(10, 6))
        plt.scatter(X, y, color='blue', label='Data points')
        plt.title(f'{feature} vs {target}')
        plt.xlabel(feature)
        plt.ylabel(target)
        plt.legend()
        plt.show()
        display(Markdown(f"### Visualization of {feature}"))
        display(Markdown(f"This scatter plot shows the relationship between the selected feature **{feature}** and the target variable **{target}**."))

# Button to visualize data
visualize_data_button = widgets.Button(description="Visualize Data")
visualize_data_button.on_click(visualize_data)

# Display the feature dropdown and target input
display(Markdown("## Step 3: Data Preprocessing"))
display(feature_dropdown)
display(target_input)
display(preprocess_dropdown)
display(columns_to_drop)
display(columns_to_fill)
display(fill_method_dropdown)
display(categorical_columns)
display(encoding_method_dropdown)
display(visualize_data_button)

# Function to drop selected columns
def drop_selected_columns(b):
    global data
    if columns_to_drop.value:
        data.drop(columns=list(columns_to_drop.value), inplace=True)
        display(Markdown("### Dropped Selected Columns"))
        display(data.head())

drop_columns_button = widgets.Button(description="Drop Columns")
drop_columns_button.on_click(drop_selected_columns)
display(drop_columns_button)

# Function to fill missing values
def fill_missing_values(b):
    global data
    if columns_to_fill.value:
        for column in columns_to_fill.value:
            if fill_method_dropdown.value == 'Mean':
                data[column].fillna(data[column].mean(), inplace=True)
            elif fill_method_dropdown.value == 'Median':
                data[column].fillna(data[column].median(), inplace=True)
            elif fill_method_dropdown.value == 'Mode':
                data[column].fillna(data[column].mode()[0], inplace=True)
            elif fill_method_dropdown.value == 'Forward Fill':
                data[column].fillna(method='ffill', inplace=True)
            elif fill_method_dropdown.value == 'Backward Fill':
                data[column].fillna(method='bfill', inplace=True)
        display(Markdown("### Filled Missing Values"))
        print(data.isnull().sum())
        display(data.head())
fill_missing_values_button = widgets.Button(description="Fill Missing Values")
fill_missing_values_button.on_click(fill_missing_values)
display(fill_missing_values_button)


# Function to handle categorical data
def handle_categorical_data(b):
    global data
    categorical_cols = list(categorical_columns.value)
    if categorical_cols:
        if encoding_method_dropdown.value == 'Label Encoding':
            label_encoder = LabelEncoder()
            for col in categorical_cols:
                data[col] = label_encoder.fit_transform(data[col])
        elif encoding_method_dropdown.value == 'One-Hot Encoding':
            data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)
        display(Markdown("### Handled Categorical Data"))
        display(data.head())

handle_categorical_data_button = widgets.Button(description="Handle Categorical Data")
handle_categorical_data_button.on_click(handle_categorical_data)
display(handle_categorical_data_button)

# Step 3: Choosing the Right Model


# Create a dropdown widget for selecting the model type (Regression or Classification)
model_type_dropdown = widgets.Dropdown(
    options=['Regression', 'Classification'],
    value='Regression',
    description='Model Type:',
    disabled=False,
)

# Create a dropdown widget for selecting the model
model_dropdown = widgets.Dropdown(
    options=[],
    description='Model:',
    disabled=False,
)

# Function to update the model dropdown based on the model type
def update_model_dropdown(change):
    if model_type_dropdown.value == 'Regression':
        model_dropdown.options = ['Linear Regression', 'Ridge', 'Lasso', 'Elastic Net']
    else:
        model_dropdown.options = ['Logistic Regression', 'KNN', 'SVM', 'Decision Tree', 'Random Forest']

model_type_dropdown.observe(update_model_dropdown, names='value')

# Display the model type dropdown and model dropdown
display(Markdown("## Step 3: Choosing the Right Model"))
display(model_type_dropdown)
display(model_dropdown)

# Step 4: Training the Model
# Create a float input widget for entering the alpha value (for regularization models)
alpha_input = widgets.FloatText(
    value=1.0,
    description='Alpha:',
    disabled=True,
)

# Function to perform Regularization (Ridge, Lasso, Elastic Net)
def perform_regularization():
    global X_train, X_test, y_train, y_test, model, y_pred
    feature = feature_dropdown.value
    target = target_input.value
    preprocess = preprocess_dropdown.value
    method = model_dropdown.value
    alpha = alpha_input.value

    X = data.drop(columns=[target]).values
    y = data[target].values

    # Preprocess the data
    if preprocess == 'Standard Scaler':
        scaler = StandardScaler()
        X = scaler.fit_transform(X)
    elif preprocess == 'Min-Max Scaler':
        scaler = MinMaxScaler()
        X = scaler.fit_transform(X)
    elif preprocess == 'Robust Scaler':
        scaler = RobustScaler()
        X = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

    if method == 'Ridge':
        model = Ridge(alpha=alpha)
    elif method == 'Lasso':
        model = Lasso(alpha=alpha)
    elif method == 'Elastic Net':
        model = ElasticNet(alpha=alpha)
    else:
        model = LinearRegression()

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

# Function to perform Classification
def perform_classification():
    global X_train, X_test, y_train, y_test, model, y_pred
    feature = feature_dropdown.value
    target = target_input.value
    preprocess = preprocess_dropdown.value
    method = model_dropdown.value

    X = data.drop(columns=[target]).values
    y = data[target].values

    # Preprocess the data
    if preprocess == 'Standard Scaler':
        scaler = StandardScaler()
        X = scaler.fit_transform(X)
    elif preprocess == 'Min-Max Scaler':
        scaler = MinMaxScaler()
        X = scaler.fit_transform(X)
    elif preprocess == 'Robust Scaler':
        scaler = RobustScaler()
        X = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

    if method == 'Logistic Regression':
        model = LogisticRegression()
    elif method == 'KNN':
        model = KNeighborsClassifier()
    elif method == 'SVM':
        model = SVC()
    elif method == 'Decision Tree':
        model = DecisionTreeClassifier()
    elif method == 'Random Forest':
        model = RandomForestClassifier()
    else:
        model = LogisticRegression()

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

# Update the visibility of alpha input based on the selected model
def update_alpha_input(change):
    if model_dropdown.value in ['Ridge', 'Lasso', 'Elastic Net']:
        alpha_input.disabled = False
    else:
        alpha_input.disabled = True

model_dropdown.observe(update_alpha_input, names='value')

# Display the alpha input
display(alpha_input)
#train model
def train_model(b):
    if model_type_dropdown.value == 'Regression':
        if model_dropdown.value in ['Ridge', 'Lasso', 'Elastic Net']:
            perform_regularization()
        else:
            perform_regression()
    else:
        perform_classification()

# Button to train the model
train_model_button = widgets.Button(description="Train Model")
train_model_button.on_click(train_model)
display(train_model_button)

# Step 6: Evaluating the Model
# Function to evaluate the model
def evaluate_model(b):
    global X_train, X_test, y_train, y_test, y_pred
    if model is None:
        display(Markdown("### Error: Model is not trained."))
        return

    if model_type_dropdown.value == 'Regression':
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        display(Markdown("### Model Evaluation"))
        display(Markdown(f"**Mean Squared Error:** {mse}"))
        display(Markdown(f"**R^2 Score:** {r2}"))

        plt.figure(figsize=(10, 6))
        plt.scatter(X_train[:, 0], y_train, color='blue', label='Training data')
        plt.plot(X_train[:, 0], model.predict(X_train), color='red', label='Regression line')
        plt.title(f'{model_dropdown.value} Regression on Training Data')
        plt.xlabel(feature_dropdown.value)
        plt.ylabel(target_input.value)
        plt.legend()
        plt.show()

        plt.figure(figsize=(10, 6))
        plt.scatter(X_test[:, 0], y_test, color='blue', label='Testing data')
        plt.plot(X_test[:, 0], y_pred, color='red', label='Regression line')
        plt.title(f'{model_dropdown.value} Regression on Test Data')
        plt.xlabel(feature_dropdown.value)
        plt.ylabel(target_input.value)
        plt.legend()
        plt.show()
    else:
        accuracy = accuracy_score(y_test, y_pred)
        display(Markdown("### Model Evaluation"))
        display(Markdown(f"**Accuracy:** {accuracy}"))
        display(Markdown(f"**Classification Report:**\n {classification_report(y_test, y_pred)}"))

evaluate_model_button = widgets.Button(description="Evaluate Model")
evaluate_model_button.on_click(evaluate_model)
#step4 Cross validation
# Step 5: Cross-Validation
# Create dropdowns for selecting cross-validation method
cv_method_dropdown = widgets.Dropdown(
    options=['K-Fold', 'Stratified K-Fold', 'Leave-One-Out'],
    value='K-Fold',
    description='CV Method:',
    disabled=False,
)

# Function to perform cross-validation
def perform_cross_validation(b):
    global data, model
    if data is not None and model_dropdown.value:
        X = data.drop(columns=[target_input.value])
        y = data[target_input.value]

        if cv_method_dropdown.value == 'K-Fold':
            cv = KFold(n_splits=5, shuffle=True, random_state=42)
        elif cv_method_dropdown.value == 'Stratified K-Fold':
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        elif cv_method_dropdown.value == 'Leave-One-Out':
            cv = LeaveOneOut()

        scores = []
        for train_index, test_index in cv.split(X, y):
            X_train, X_test = X.iloc[train_index], X.iloc[test_index]
            y_train, y_test = y.iloc[train_index], y.iloc[test_index]
            model.fit(X_train, y_train)
            predictions = model.predict(X_test)
            if model_type_dropdown.value == 'Regression':
                scores.append(mean_squared_error(y_test, predictions))
            else:
                scores.append(accuracy_score(y_test, predictions))

        if model_type_dropdown.value == 'Regression':
            display(Markdown(f"### Cross-Validation Scores (MSE)"))
            display(Markdown(f"Mean MSE: {np.mean(scores):.4f}"))
            display(Markdown(f"Standard Deviation of MSE: {np.std(scores):.4f}"))
        else:
            display(Markdown(f"### Cross-Validation Scores (Accuracy)"))
            display(Markdown(f"Mean Accuracy: {np.mean(scores):.4f}"))
            display(Markdown(f"Standard Deviation of Accuracy: {np.std(scores):.4f}"))
display(Markdown("## Step 5: Cross Validation"))
perform_cross_validation_button = widgets.Button(description="Perform Cross-Validation")
perform_cross_validation_button.on_click(perform_cross_validation)
display(cv_method_dropdown)
display(perform_cross_validation_button)

# Display the evaluate model button
display(Markdown("## Step 6: Evaluating the Model"))
display(evaluate_model_button)

# Step 7: Hyperparameter Tuning and Optimization
# Function to perform hyperparameter tuning
def perform_hyperparameter_tuning(b):
    feature = feature_dropdown.value
    target = target_input.value

    method = model_dropdown.value

    X = data.drop(columns=[target]).values
    y = data[target].values



    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

    if model_type_dropdown.value == 'Regression':
        if method == 'Ridge':
            param_grid = {'alpha': np.logspace(-4, 4, 50)}
            grid_search = GridSearchCV(Ridge(), param_grid, cv=5)
        elif method == 'Lasso':
            param_grid = {'alpha': np.logspace(-4, 4, 50)}
            grid_search = GridSearchCV(Lasso(), param_grid, cv=5)
        elif method == 'Elastic Net':
            param_grid = {'alpha': np.logspace(-4, 4, 50), 'l1_ratio': np.linspace(0, 1, 10)}
            grid_search = GridSearchCV(ElasticNet(), param_grid, cv=5)
        else:
            grid_search = None
    else:
        if method == 'Logistic Regression':
            param_grid = {'C': np.logspace(-4, 4, 50)}
            grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)
        elif method == 'KNN':
            param_grid = {'n_neighbors': np.arange(1, 31)}
            grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)
        elif method == 'SVM':
            param_grid = {'C': np.logspace(-4, 4, 50), 'kernel': ['linear', 'rbf']}
            grid_search = GridSearchCV(SVC(), param_grid, cv=5)
        elif method == 'Decision Tree':
            param_grid = {'max_depth': np.arange(1, 21)}
            grid_search = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)
        elif method == 'Random Forest':
            param_grid = {'n_estimators': [10, 50, 100, 200], 'max_depth': np.arange(1, 21)}
            grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
        else:
            grid_search = None

    if grid_search:
        grid_search.fit(X_train, y_train)
        best_params = grid_search.best_params_
        best_score = grid_search.best_score_
        display(Markdown("### Hyperparameter Tuning Results"))
        display(Markdown(f"**Best Parameters:** {best_params}"))
        display(Markdown(f"**Best Cross-Validation Score:** {best_score}"))

hyperparameter_tuning_button = widgets.Button(description="Perform Hyperparameter Tuning")
hyperparameter_tuning_button.on_click(perform_hyperparameter_tuning)

# Display the hyperparameter tuning button
display(Markdown("## Step 7: Hyperparameter Tuning and Optimization"))
display(hyperparameter_tuning_button)

# Step 8: Predictions and Deployment
# Function to create input widgets for each feature
def update_feature_widgets():
    global feature_widgets
    feature_widgets = {}
    if data is not None:
        for column in data.columns[:-1]:
            feature_widgets[column] = widgets.FloatText(
                value=0.0,
                description=column,
                disabled=False,
            )
        display(widgets.VBox(list(feature_widgets.values())))

# Function to make predictions with new input values
def make_predictions(b):
    global model, label_encoder
    if model is None:
        display(Markdown("### Error: Model is not trained."))
        return

    input_data = np.array([[widget.value for widget in feature_widgets.values()]])
    prediction = model.predict(input_data)
    if label_encoder:
        prediction = label_encoder.inverse_transform(prediction)
    display(Markdown(f"### Prediction for Input:"))
    for col, widget in feature_widgets.items():
        display(Markdown(f"**{col}:** {widget.value}"))
    display(Markdown(f"**Predicted Value:** {prediction[0]}"))

predict_button = widgets.Button(description="Make Prediction")
predict_button.on_click(make_predictions)

# Display the prediction widgets and button
display(Markdown("## Step 8: Predictions and Deployment"))
display(predict_button)